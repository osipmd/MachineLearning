Линейная регрессия

1) Определение
2) Метод наименьших квадратов
3) Регуляризация
    - Yandex
    Регуляризация - способ борьбы с переобучением в линейных моделях.
    Предположим, что мы решаем оптимизационную задачу - минимизируем среднеквадратичное отклонение. Мы нашли решение.

    Большие веса в линейной модели - высокий риск случившегося переобучения. Давайте будем штрафовать за это.
    То есть к функционалу ошибки добавляем слагаемое, которое штрафует за сложность модели.
    Q(w, x) - функционал ошибки
    Квадратичный регуляризатор - ||w||^2 = Σ(Wj)^2
    И теперь новый функционал - Q(w,x) + λ*||w||^2  -> min (Так мы стремимся уменьшить функционал ошибки и при этом
    не иметь больших весов при признаках)
    При этом в модели появляется новый параметр λ - коэффициент регуляризации

    - Лекции
    Гипотеза : W колеблется (имеет большие веса) из-за переобучения. Это происходит из-за мультиколинеарности,
    влзникающей между различными признаками при увеличении кол-ва признаков.

    Основная идея: обрезать w норму
    Добавлять регуляризационный штраф для нормы весов
    Qt(aw, Tl)


4) Нормализация - почему лучше
5) Генетический алгоритм